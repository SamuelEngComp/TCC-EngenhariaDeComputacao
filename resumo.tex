\chapter*{Resumo}

\noindent Big Data é o termo utilizado para definir grandes volumes de dados, os quais os bancos de dados relacionais não conseguiriam armazenar de forma estruturada. Tendo em vista que os dados provenientes do Big Data são, em sua grande maioria, dados não estruturados. Para armazenar tais dados foi criado o \textit{framework} Apache Hadoop, o qual possui duas ferramentas essencias para o armazenamento e processamento de Big Data, são elas: MapReduce e HDFS. Sabendo que grandes empresas de tecnologia, tais como Facebook, Google e Amazon, utilizam Big Data, ou seja, utilizam dados dos próprios usuários para direcionar propagandas de marketing específicas para cada grupo de usuários que compartilham interesses semelhantes e, fazendo isso, gerar receita e lucro, houve uma preocupação quanto à segurança desses dados, o que levanta o seguinte questionamento: eles estão devidamente protegidos contra ataques? Tendo isso em mente, este trabalho propõe algumas boas práticas de segurança para manter os dados seguros e 'blindar' o ambiente Hadoop, verificando suas falhas e propondo práticas de segurança pertinentes para evitar perdas e roubos. A metodologia aplicada consiste na exploração de um ambiente virtualizado com três cenários de teste, nos quais são verificadas as vulnerabilidades encontradas em um ambiente Hadoop e como é possível corrigi-las.

\vspace{1.5ex}

{\bf Palavras-chave}: Big Data, Segurança da informação, 
Banco de dados, linguagem de programação java, frameworks.